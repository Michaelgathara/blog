---
title: "How Platforms Know if An Image/Video is AI-Made"
date: "2025-10-04"
path: "/by_ai_or_human"
desc: "How platforms know if an image or video is made by AI, Cryptographic Provenance"
---

# Introduction
Platforms are increasingly adopting cryptographic provenance systems to verify if media was physically captured by a camera or generated artificially. At the core of this is <strong>public-key cryptography</strong> at the image sensor level, cameras are equipped with secure chips holding private keys to digitally sign photos as they are taken. For example, Leica's latest cameras implement the <i>Coalition for Content Provenance and Authenticity (C2PA)</i> standard: each photo from a [Leica M11-P](https://leica-camera.com/en-US/press/new-leica-m11-p) includes a <strong>forgery-proof digital signature</strong> documenting the camera model, manufacturer, and a hash of the image content. This signature is stored in metadata to form part of a <strong>[C2PA manifest](https://spec.c2pa.org/specifications/specifications/2.2/specs/C2PA_Specification.html)</strong> (which is basically a signed JSON) that is stuck with the file and allows anyone to verify if the image has been altered. Verification tools can check the signature against the camera's public certificate to confirm the photo's origin and detect any post-capture edits. 

## What happens if the metadata is tampered with?
Crucially, the provenance metadata is tamper-evident. If even one bit of the image or its signed data is changed, the signature check will fail. This makes the image's authenticity auditable. This makes it easy for organizations such as the news and social platforms to reject or flag any content whose signatures don't verify. Companies like Sony, Nikon, and Google are incorporating this at the device level. For instance, Google's Pixel 10 (the 2025 Pixel) [automatically attach Content Credentials](https://security.googleblog.com/2025/09/pixel-android-trusted-images-c2pa-content-credentials.html#:~:text=At%20Made%20by%20Google%202025%2C,towards%20greater%20digital%20media%20transparency) (C2PA-compliant signatures and metadata) to every photo taken with the built-in camera app. 


## Lack of Credentials 
Given the above approach, one emerging approach for signaling AI generated content is the <strong>absence of trusted credentials</strong>. Rather than naively classifying content as "AI" vs "Human-made" the goal is divide media into 

1. Content with verifiable proof of origin
2. content lacking such proof

In other words, if a photo or video comes with a valid Content Credential showing it was captured by a real camera and was unedited, one can trust it regardless of its appearance. Conversely, if an image is missing credentials, it does not automatically mean it's fake, but that it carries no <i>proof of authenticity</i> and thus should be treated with suspicion. [Google explicitly advocates](https://security.googleblog.com/2025/09/pixel-android-trusted-images-c2pa-content-credentials.html#:~:text=Instead%20of%20categorizing%20digital%20content,media%20that%20doesn%27t) for this model: "Either media comes with verifiable proof of how it was made, or it doesn’t". In an ideal future, most legitimate photos/videos will come with cryptographic provenance; anything without it would by default raise suspicions of AI manipulation or source. Today, we’re in a transition: many devices and apps don’t yet sign content, so a missing credential is not uncommon. As adoption grows, however, platforms can increasingly interpret a lack of Content Credentials as a red flag (or at least prompt extra scrutiny), while positively identifying real, unaltered media via present credentials.

## Self-Disclosure by AI Tools via Metadata
Leading AI content generators themselves are beginning to self-disclose AI-generated media through metadata and cryptographic attestations. Many generative AI tools now automatically attach a “made by AI” label in the output file’s metadata, often using the same C2PA Content Credentials format as digital cameras. For example, Adobe’s generative image tools ([Firefly](https://helpx.adobe.com/firefly/web/get-started/learn-the-basics/adobe-firefly-faq.html#:~:text=creators%20can%20add%20tamper,or%20not%20to%20trust%20it)) embed a signed Content Credential whenever you export an AI-created image. These credentials explicitly indicate that an AI tool was used in the creation. In practice, this means if you generate an image with Firefly and share it, anyone can inspect its content credentials (using a verify tool or browser extension) and see an entry like “Created with: Adobe Firefly (AI)” along with timestamps and potentially the prompt or model info. Adobe, as a founder of the Content Authenticity Initiative (CAI), has baked this into Creative Cloud apps. This opt-in transparency by the generator is a powerful signal: it’s cryptographically signed by Adobe’s keys, so it can’t be forged or removed without detection (unless stripped entirely, which we’ll address later). Signed AI attestations ensure there are no false positives: if the metadata says “AI-generated by Adobe Firefly,” you can trust that claim.

OpenAI is similarly tagging the outputs of its image models. Images [created with DALL·E 3](https://petapixel.com/2024/02/08/ai-images-generated-on-dall-e-now-contain-the-content-authenticity-tag/#:~:text=AI%20images%20generated%20on%20OpenAI%E2%80%99s,C2PA) (such as through ChatGPT’s image generation interface) now come with C2PA metadata stating they were generated by OpenAI’s model. In fact, OpenAI joined the C2PA steering committee and began adding Content Credentials to all DALL·E 3 images by late 2023. The embedded manifest in a DALL·E image includes details like the tool (“OpenAI DALL·E”), actions (e.g. “Generated from text prompt”), and a unique signature. Even if the image is edited afterwards (e.g. using OpenAI’s built-in image editor or another AI edit), the content credential is updated – preserving a chain of provenance that records the edit and the tool used. 